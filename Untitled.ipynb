{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80a40483-5344-4272-86e8-e3402bbd27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_1\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_2\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_3\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_4\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_5\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_6\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_7\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_8\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_9\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_10\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_11\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_12\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_13\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_14\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_15\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_16\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_17\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_18\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_19\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_20\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_21\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_22\n",
      "Created: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_23\n",
      "File split successfully into 23 chunks!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def split_file(file_path, output_dir, chunk_size):\n",
    "    \"\"\"\n",
    "    Splits a file into smaller chunks.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file to be split.\n",
    "        output_dir (str): Directory where the chunks will be saved.\n",
    "        chunk_size (int): Size of each chunk in bytes.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the output directory exists\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        \n",
    "        # Open the file to read in binary mode\n",
    "        with open(file_path, 'rb') as file:\n",
    "            chunk_number = 1\n",
    "            while True:\n",
    "                # Read a chunk of the file\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                \n",
    "                # Write the chunk to a new file\n",
    "                chunk_file_path = os.path.join(output_dir, f\"chunk_{chunk_number}\")\n",
    "                with open(chunk_file_path, 'wb') as chunk_file:\n",
    "                    chunk_file.write(chunk)\n",
    "                \n",
    "                print(f\"Created: {chunk_file_path}\")\n",
    "                chunk_number += 1\n",
    "\n",
    "        print(f\"File split successfully into {chunk_number - 1} chunks!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file splitting: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the file to be split and output directory\n",
    "    file_to_split = \"/Users/roshin/Documents/Slides/templates/mlx/model.safetensors\"\n",
    "    output_directory = \"/Users/roshin/Documents/Slides/templates/mlx/output\"\n",
    "    \n",
    "    # Define the chunk size in bytes (e.g., 10 MB = 10 * 1024 * 1024 bytes)\n",
    "    chunk_size_in_bytes = 90 * 1024 * 1024  # 10 MB\n",
    "    \n",
    "    # Call the function to split the file\n",
    "    split_file(file_to_split, output_directory, chunk_size_in_bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df1590bd-ecde-42ac-9406-b054ac8ead34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_1\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_2\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_3\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_4\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_5\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_6\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_7\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_8\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_9\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_10\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_11\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_12\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_13\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_14\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_15\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_16\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_17\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_18\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_19\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_20\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_21\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_22\n",
      "Merged: /Users/roshin/Documents/Slides/templates/mlx/output/chunk_23\n",
      "File merged successfully into: /Users/roshin/Documents/Slides/templates/mlx/model_merged new.safetensors\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def merge_chunks(output_file_path, chunks_dir):\n",
    "    \"\"\"\n",
    "    Merges chunks back into the original file.\n",
    "    \n",
    "    Args:\n",
    "        output_file_path (str): Path where the merged file will be saved.\n",
    "        chunks_dir (str): Directory containing the chunk files to merge.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a sorted list of all chunk files\n",
    "        chunk_files = sorted(\n",
    "            [f for f in os.listdir(chunks_dir) if f.startswith(\"chunk_\")],\n",
    "            key=lambda x: int(x.split('_')[-1])  # Sort by chunk number\n",
    "        )\n",
    "        \n",
    "        # Merge all chunks into the output file\n",
    "        with open(output_file_path, 'wb') as output_file:\n",
    "            for chunk_file in chunk_files:\n",
    "                chunk_file_path = os.path.join(chunks_dir, chunk_file)\n",
    "                with open(chunk_file_path, 'rb') as cf:\n",
    "                    output_file.write(cf.read())\n",
    "                print(f\"Merged: {chunk_file_path}\")\n",
    "\n",
    "        print(f\"File merged successfully into: {output_file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file merging: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define the directory containing the chunks and the output file\n",
    "    chunks_directory = \"/Users/roshin/Documents/Slides/templates/mlx/output\"\n",
    "    merged_file_path = \"/Users/roshin/Documents/Slides/templates/mlx/model_merged new.safetensors\"\n",
    "    \n",
    "    # Call the function to merge chunks\n",
    "    merge_chunks(merged_file_path, chunks_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fc4d8c-d75e-4306-a0d6-facfa0643271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx import MLXModel, MLXTokenizer\n",
    "\n",
    "def load_mlx_llm_model(model_path):\n",
    "    \"\"\"\n",
    "    Loads a local MLX LLM model and tokenizer.\n",
    "    \n",
    "    Args:\n",
    "        model_path (str): Path to the directory containing the MLX model files.\n",
    "        \n",
    "    Returns:\n",
    "        model, tokenizer: Loaded MLX model and tokenizer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the MLX model and tokenizer\n",
    "        model = MLXModel.from_pretrained(model_path)\n",
    "        tokenizer = MLXTokenizer.from_pretrained(model_path)\n",
    "        print(\"MLX LLM model and tokenizer loaded successfully!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MLX LLM model: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_length=100):\n",
    "    \"\"\"\n",
    "    Generates a response to a given prompt using the MLX LLM model.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): The input prompt text.\n",
    "        model (MLXModel): Loaded MLX model.\n",
    "        tokenizer (MLXTokenizer): Tokenizer for the MLX model.\n",
    "        max_length (int): Maximum length of the generated response.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response text.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Tokenize the input prompt\n",
    "        inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate a response using the model\n",
    "        outputs = model.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            no_repeat_ngram_size=2,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        # Decode the output tokens\n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Path to the local MLX model directory\n",
    "    model_directory = \"./mlx_llm_model\"  # Replace with the actual path to your model\n",
    "    \n",
    "    # Load the MLX LLM model and tokenizer\n",
    "    mlx_model, mlx_tokenizer = load_mlx_llm_model(model_directory)\n",
    "    \n",
    "    if mlx_model and mlx_tokenizer:\n",
    "        # Example prompt\n",
    "        example_prompt = \"What are the applications of large language models?\"\n",
    "        \n",
    "        # Generate a response\n",
    "        response = generate_response(example_prompt, mlx_model, mlx_tokenizer)\n",
    "        \n",
    "        if response:\n",
    "            print(\"Generated Response:\")\n",
    "            print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c300fd-2066-4537-8ea6-ca3275deeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "model_path = \"mlx-community/Mistral-7B-Instruct-v0.2-4bit\"\n",
    "prompt = prompt_builder(\"Great content, thank you!\")\n",
    "max_tokens = 140\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/Mistral-7B-Instruct-v0.2-4bit\")\n",
    "response = generate(model, tokenizer, prompt=prompt, max_tokens = max_tokens,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e2cbc-1650-4316-accf-10660b7f099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Step 1: Web Crawler to Scrape Links and Sub-links\n",
    "def crawl_website(base_url, max_depth=2):\n",
    "    visited_urls = set()\n",
    "    content_data = []\n",
    "\n",
    "    def scrape_page(url, depth):\n",
    "        if depth > max_depth or url in visited_urls:\n",
    "            return\n",
    "        \n",
    "        print(f\"Scraping: {url}\")\n",
    "        visited_urls.add(url)\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                return\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text content\n",
    "            page_text = \" \".join([p.get_text() for p in soup.find_all('p')])\n",
    "            if page_text.strip():\n",
    "                content_data.append({\"url\": url, \"content\": page_text.strip()})\n",
    "            \n",
    "            # Find all sub-links on the page\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                absolute_link = urljoin(url, link['href'])\n",
    "                # Only follow links within the same domain\n",
    "                if base_url in absolute_link and absolute_link not in visited_urls:\n",
    "                    scrape_page(absolute_link, depth + 1)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape {url}: {e}\")\n",
    "\n",
    "    # Start scraping from the base URL\n",
    "    scrape_page(base_url, depth=0)\n",
    "    return content_data\n",
    "\n",
    "# Step 2: Save Data in LLM Training-Friendly Format\n",
    "def save_data_to_llm_format(data, output_file=\"website_data.jsonl\"):\n",
    "    \"\"\"\n",
    "    Save data in JSONL format where each line represents a training example:\n",
    "    {\n",
    "        \"url\": \"page_url\",\n",
    "        \"content\": \"text_content\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    import json\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for entry in data:\n",
    "            json.dump(entry, file)\n",
    "            file.write('\\n')\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    base_url = \"https://example.com\"  # Replace with the website you want to scrape\n",
    "    max_depth = 2  # Define the depth of crawling\n",
    "\n",
    "    # Crawl the website\n",
    "    crawled_data = crawl_website(base_url, max_depth=max_depth)\n",
    "    print(f\"Scraped {len(crawled_data)} pages.\")\n",
    "\n",
    "    # Save data in JSONL format\n",
    "    save_data_to_llm_format(crawled_data, output_file=\"training_data.jsonl\")\n",
    "    print(\"Data saved in LLM training-friendly format: training_data.jsonl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
